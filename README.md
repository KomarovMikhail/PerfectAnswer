# Анализатор ответов на сходство с "идеальным"

### Краткое описание

Программа берет ответы из excel-таблицы считает степень их сходства с идеальным ответом и пишет результат в таблицу.


Файл и с идеальным ответом: `perfect_answer`  
Исходная таблица с ответами: `IT_Analytics_test_1.xlsx`  
Таблица с результатами работы: `result.xlsx`  
Файл со стоп словами: `stops`  
Основная логика реализована в модуле `TextAnalyzer.py`

Для запуска программы набрать в консоли:

`$ python3 main.py`

### Мера сходства

При оценке ответов нам важны:
1. Структура. Она достигается за счет нумерации (1, 2, 3) и разделения внутри
нумерации на риск, описание, мера.
2. Полнота ответа. Должны быть написаны и риск, и его описание, и мера.
3. Контент. Тут важно, что именно написано по каждому пункту.

Исходя из вышеописанных утверждений, меру сходства ответа с "идеальным" будем вычислять по следующей формуле:

```
Measure = (Structure * 0.5 + Completeness * 0.5 + Content * 2) / 3
```

Переменные `Structure`, `Completeness`, `Content` лежат в отрезке [0,1]. Вклад `Structure` и `Completeness`
в итоговый ответ выбран меньше из соображений, что в первую очередь важен именно контент ответа. 

#### Stucture

```
Structure = (One + Two + Three) / 3
```

Здесь `One`, `Two` и `Three` являются индикаторами того, что в ответе присутствуют отдельно стоящие
цыфры 1, 2 и 3 соответственно, т.к. если участник записал их в ответ, то вероятнее всего они служат
разделителями между частями ответа, то есть задают некоторую структуру.

#### Completeness

```
Completeness = (Risk + Solution) / 6 
```

Здесь `Risk` и `Solution` равны по `3`, если в ответе присутствуют слова "риск" и "мера" ка минимум по три раза 
каждое. Если данные слова присутствуют в тексте меньшее количество раз - значение этих коэффициентов равно
количеству вхождений соответствующих слов.
Подобный коэффициент введен из соображений, что если эти слова присутствуют в тексте, то, вероятнее всего, они
предшествуют описанию самого риска и меры по его устранению.

#### Content

Значение коэффициента `Content` вычисляется с помощью алгоритма LSA (Латентно-Семантического анализа текста).
Шаги алгоритма:
1. Приведение слов к нижнему регистру и нормальной форме, удаление стоп-слов
2. Составление tf-idf матрицы для слов/ответов, удаление из нее слов, встречающихся только один раз
3. SVD разложение полученной матрицы с выделением наиболее значимых тем.
4. Взятие косинусного расстояния между ответами в полученном семантическом пространстве, которое и будет являться значением 
`Content`


##### P.S.

При запуске main.py вылетают некоторые варнинги, связанные с делением на ноль (некоторые строки исходной таблицы
с ответами пустые, и для них хочется в результате возвращать нулевое сходство с идеальным ответом).
Numpy в этом случае возвращает значение numpy.NaN. Это ситуация обработанна в коде последующей
заменой NaN-ов нулями. 

